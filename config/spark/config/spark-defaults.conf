#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"

#spark.sql.catalog.iceberg   org.apache.iceberg.spark.SparkCatalog
#spark.sql.catalog.iceberg.type   hive
#spark.sql.catalog.iceberg.uri   thrift://localhost:9083  
#spark.sql.catalog.iceberg.warehouse   hdfs:///user/hive/warehouse

#spark.sql.extensions   org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
#spark.sql.catalog.spark_catalog   org.apache.iceberg.spark.SparkSessionCatalog
#spark.sql.catalog.spark_catalog.type   hive
#spark.sql.catalog.local   org.apache.iceberg.spark.SparkCatalog
#spark.sql.catalog.local.type   hadoop
#spark.sql.catalog.local.warehouse   home/hoang/warehouse
#spark.sql.catalog.local                              org.apache.iceberg.spark.SparkCatalog
#spark.sql.catalog.local.type                         hadoop
spark.sql.catalog.local.warehouse                    hdfs://localhost:9000
#spark.sql.defaultCatalog                             iceberg

spark.hadoop.dfs.replication                          1
# IMPORTANT: chu y chinh sua duong dan
spark.driver.extraClassPath                          /opt/spark/spark-3.3.0-bin-hadoop3/extra_lib_jars/*  
# IMPORTANT: chu y chinh sua duong dan
spark.executor.extraClassPath                  	     /opt/spark/spark-3.3.0-bin-hadoop3/extra_lib_jars/*   
spark.sql.extensions                                 org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.spark_catalog                      org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type                 hive
# IMPORTANT: chu y doi host
spark.sql.catalog.spark_catalog.warehouse   hdfs://localhost:9000  
spark.sql.catalog.iceberg.type   hive
spark.sql.catalog.iceberg        org.apache.iceberg.spark.SparkCatalog
# IMPORTANT: chu y doi host
spark.sql.catalog.iceberg.warehouse   hdfs://localhost:9000


spark.sql.catalog.hive   org.apache.spark.sql.hive.HiveCatalog
# IMPORTANT: chu y doi host
spark.sql.catalog.hive.uri   thrift://localhost:9083   
# IMPORTANT: chu y doi host
spark.sql.catalog.hive.warehouse   hdfs://localhost:9000/user/hive/warehouse 
# IMPORTANT: chu y doi host
spark.hadoop.hive.metastore.uris   thrift://localhost:9083 

spark.sql.hive.thriftServer.singleSession   true

spark.hadoop.hive.server2.thrift.bind.host   0.0.0.0
spark.hadoop.hive.server2.thrift.port   8989
spark.sql.hive.metastore.sharedPrefixes   com.mysql.jdbc
spark.hadoop.fs.permissions.umask-mode 000


#spark.sql.execution.timeout 30s
spark.sql.broadcastTimeout 30s
spark.sql.adaptive.enabled true
spark.sql.adaptive.autoBroadcastJoinThreshold 0
spark.sql.join.preferSortMergeJoin true

#spark.driver.memory   4g
#spark.executor.memory   2g
#spark.executor.cores   2
#spark.dynamicAllocation.executorIdleTimeout	1min
#spark.dynamicAllocation.schedulerBacklogTimeout	1s
#spark.sql.thriftServer.interruptOnCancel true
#spark.task.reaper.enabled	true
#spark.task.reaper.pollingInterval 10s 
#spark.task.reaper.killTimeout	3s
#spark.sql.ui.retainedExecutions 20000
#spark.eventLog.enabled false
#spark.worker.cleanup.enabled true
#spark.worker.cleanup.interval	5
#spark.sql.autoBroadcastJoinThreshold   10MB
#spark.sql.adaptive.logLevel   DEBUG


